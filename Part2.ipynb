{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "HO4oEXOJTQ5X",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "outputId": "b6ed44c0-65b2-46fc-db6c-da399b777faf"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'attention'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-0457661a60e2>\u001b[0m in \u001b[0;36m<cell line: 20>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mattention\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAttention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'attention'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "# Import libraries\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Dense, Embedding, LSTM, Attention, Input, Flatten, Lambda, dot, Activation, concatenate\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.utils import to_categorical, plot_model\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from tensorflow.keras.constraints import NonNeg\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib import cm\n",
        "from matplotlib.colors import rgb2hex\n",
        "from sklearn.preprocessing import minmax_scale\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from skimage import io\n",
        "from IPython.core.display import HTML\n",
        "import os\n",
        "import time\n",
        "from attention import Attention"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "M3bvp02cE6yu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Path to Harry Potter text\n",
        "\n",
        "path_to_file = tf.keras.utils.get_file('harrypotter.txt', \"https://raw.githubusercontent.com/amephraim/nlp/master/texts/J.%20K.%20Rowling%20-%20Harry%20Potter%201%20-%20Sorcerer's%20Stone.txt\")"
      ],
      "metadata": {
        "id": "_1e4CE6ye7ML"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read, then decode for py2 compat.\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "# length of text is the number of characters in it\n",
        "print(f'Length of text: {len(text)} characters')"
      ],
      "metadata": {
        "id": "NPA_9s2_fCfq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Take a look at the first 250 characters in text\n",
        "print(text[:250])"
      ],
      "metadata": {
        "id": "6ZDYuFeyfCsz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The unique characters in the file\n",
        "vocab = sorted(set(text))\n",
        "print(f'{len(vocab)} unique characters')"
      ],
      "metadata": {
        "id": "QMrONiqrfCjB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting text into characters\n",
        "\n",
        "chars = tf.strings.unicode_split(text, input_encoding='UTF-8')\n",
        "chars"
      ],
      "metadata": {
        "id": "NWeUiJJEiwLq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Map characters to IDs\n",
        "\n",
        "ids_from_chars = tf.keras.layers.StringLookup(\n",
        "    vocabulary=list(vocab), mask_token=None)"
      ],
      "metadata": {
        "id": "oTVh-gh4iwPA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ids = ids_from_chars(chars)\n",
        "ids"
      ],
      "metadata": {
        "id": "SbeHPgQliwTw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Map IDs back to characters\n",
        "\n",
        "chars_from_ids = tf.keras.layers.StringLookup(\n",
        "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)"
      ],
      "metadata": {
        "id": "lh6U8RvliwXZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chars = chars_from_ids(ids)\n",
        "chars"
      ],
      "metadata": {
        "id": "GhD6SZ-aiwaQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = tf.strings.reduce_join(chars, axis=-1).numpy()\n",
        "print(output[:200])"
      ],
      "metadata": {
        "id": "VuFygSDeiwc0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to convert IDs back to text\n",
        "\n",
        "def text_from_ids(ids):\n",
        "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
      ],
      "metadata": {
        "id": "E19S7zauiwfc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert characters to IDs\n",
        "\n",
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
        "all_ids"
      ],
      "metadata": {
        "id": "PAZTUqmtiwiH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
      ],
      "metadata": {
        "id": "uxV7k0l3jyWv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Decode IDs back to characters\n",
        "\n",
        "for ids in ids_dataset.take(10):\n",
        "    print(chars_from_ids(ids).numpy().decode('utf-8'))"
      ],
      "metadata": {
        "id": "JsoyoOBqjyfF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq_length = 100"
      ],
      "metadata": {
        "id": "CL7qXMGcjyiE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Batch sequences into fixed length sequences\n",
        "\n",
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for seq in sequences.take(1):\n",
        "  print(chars_from_ids(seq))"
      ],
      "metadata": {
        "id": "nF40RiThjyk2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print text\n",
        "\n",
        "for seq in sequences.take(5):\n",
        "  print(text_from_ids(seq).numpy())"
      ],
      "metadata": {
        "id": "PRfnEg9UkJRx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function for splitting into input and target\n",
        "\n",
        "def split_input_target(sequence):\n",
        "    input_text = sequence[:-1]\n",
        "    target_text = sequence[1:]\n",
        "    return input_text, target_text"
      ],
      "metadata": {
        "id": "ls1vRYTqkJYo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform splitting operation on X_train and X_test\n",
        "X_train_input, X_train_target = split_input_target(input_text)\n",
        "X_test_input, X_test_target = split_input_target(target_text)"
      ],
      "metadata": {
        "id": "GDY0d3_ZP4H6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = sequences.map(split_input_target)\n"
      ],
      "metadata": {
        "id": "aQcC50ujkJeT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the input and target texts for the first sequence in the dataset.\n",
        "\n",
        "for input_example, target_example in dataset.take(1):\n",
        "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
        "    print(\"Target:\", text_from_ids(target_example).numpy())"
      ],
      "metadata": {
        "id": "3kCHS7oGkJgf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Training"
      ],
      "metadata": {
        "id": "dP8DodsB38CC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = (\n",
        "    dataset\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n",
        "dataset"
      ],
      "metadata": {
        "id": "ZJlS9tqyjynm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Modeling"
      ],
      "metadata": {
        "id": "SPXvK-4V3-fo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Length of the vocabulary in StringLookup Layer\n",
        "vocab_size = len(ids_from_chars.get_vocabulary()) #It is 80\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 290\n",
        "\n",
        "# Number of RNN units\n",
        "#RNN units is not random and should be chosen\n",
        "#based on careful consideration of factors such as data complexity,\n",
        "#model capacity, risk of overfitting,\n",
        "rnn_units = 384"
      ],
      "metadata": {
        "id": "OaGqfygFjyqt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RNN with rnn_layer = tf.keras.layers.GRU**"
      ],
      "metadata": {
        "id": "2bxJ4qzSXSCf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a GRU model for text generation.\n",
        "\n",
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__(self)\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True)\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x = inputs\n",
        "    x = self.embedding(x, training=training)\n",
        "    if states is None:\n",
        "      states = self.gru.get_initial_state(x)\n",
        "    x, states = self.gru(x, initial_state=states, training=training)\n",
        "    x = self.dense(x, training=training)\n",
        "\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else:\n",
        "      return x\n",
        "\n",
        "model = MyModel(\n",
        "    vocab_size=vocab_size,\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)"
      ],
      "metadata": {
        "id": "gk6JbAClku5D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get batches of input and target\n",
        "\n",
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "metadata": {
        "id": "vvcETb_3ku9K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()\n"
      ],
      "metadata": {
        "id": "JqF4d2cSkvAf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample characters from the predicted logits and convert to numpy array\n",
        "\n",
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()"
      ],
      "metadata": {
        "id": "Djq9ZPOBkvDY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_indices"
      ],
      "metadata": {
        "id": "CVy7He7ekvGA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
        "print()\n",
        "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"
      ],
      "metadata": {
        "id": "rC27JumPkvIt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get loss\n",
        "\n",
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ],
      "metadata": {
        "id": "7NABSb8tkvLL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"Mean loss:        \", example_batch_mean_loss)"
      ],
      "metadata": {
        "id": "D7xvo-GJkvNz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.exp(example_batch_mean_loss).numpy()"
      ],
      "metadata": {
        "id": "2M2rR1T5kvP4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model\n",
        "\n",
        "model.compile(optimizer='adam', loss=loss)\n"
      ],
      "metadata": {
        "id": "7qVeBxfbjytv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "metadata": {
        "id": "-IcF3KfSliC7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 20 # We need to increase the epochs as the loss is decreaseing very slowly\n",
        "#but we will not run the 40 epochs sice we have gpu limitations"
      ],
      "metadata": {
        "id": "9J2_YDt6liFy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the model\n",
        "\n",
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])\n"
      ],
      "metadata": {
        "id": "AiiHHU8XliIb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wrapper around RNN"
      ],
      "metadata": {
        "id": "_BrlqUS_Xao1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "    super().__init__()\n",
        "    self.temperature = temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
        "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        # Put a -inf at each bad index.\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices=skip_ids,\n",
        "        # Match the shape to the vocabulary\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "    # Convert strings to token IDs.\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "    # Run the model.\n",
        "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
        "                                          return_state=True)\n",
        "    # Only use the last prediction.\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    # Sample the output logits to generate token IDs.\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    # Convert from token ids to characters\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "    # Return the characters and model state.\n",
        "    return predicted_chars, states"
      ],
      "metadata": {
        "id": "ONS76GHOliLT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instance of OneStep model\n",
        "\n",
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
      ],
      "metadata": {
        "id": "DnX3aMg6liNq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate text using model\n",
        "\n",
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['Ron'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ],
      "metadata": {
        "id": "YEGlqs0TliQS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RNN with LSTR**"
      ],
      "metadata": {
        "id": "gev_MU5Xe9dF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define the MyModel class for LSTM\n",
        "class MyLSTMModel(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "        super().__init__(self)\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = tf.keras.layers.LSTM(rnn_units,\n",
        "                                         return_sequences=True,\n",
        "                                         return_state=True)\n",
        "        self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    def call(self, inputs, states=None, return_state=False, training=False):\n",
        "        x = inputs\n",
        "        x = self.embedding(x, training=training)\n",
        "        if states is None:\n",
        "            states = self.lstm.get_initial_state(x)\n",
        "        x, states_h, states_c = self.lstm(x, initial_state=states, training=training)\n",
        "        x = self.dense(x, training=training)\n",
        "\n",
        "        if return_state:\n",
        "            return x, [states_h, states_c]\n",
        "        else:\n",
        "            return x\n",
        "\n",
        "# Create an instance of the MyLSTMModel class\n",
        "lstm_model = MyLSTMModel(\n",
        "    vocab_size=vocab_size,\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)\n",
        "\n",
        "# Compile the LSTM model\n",
        "lstm_model.compile(optimizer='adam', loss=loss)\n",
        "\n",
        "# Define checkpoint callbacks for LSTM model\n",
        "checkpoint_dir_lstm = './training_checkpoints_lstm'\n",
        "checkpoint_prefix_lstm = os.path.join(checkpoint_dir_lstm, \"ckpt_{epoch}\")\n",
        "checkpoint_callback_lstm = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix_lstm,\n",
        "    save_weights_only=True)\n",
        "\n",
        "# Train the LSTM model\n",
        "history_lstm = lstm_model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback_lstm])\n",
        "\n",
        "# Define the OneStep class for LSTM\n",
        "class OneStepLSTM(tf.keras.Model):\n",
        "    def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "        super().__init__()\n",
        "        self.temperature = temperature\n",
        "        self.model = model\n",
        "        self.chars_from_ids = chars_from_ids\n",
        "        self.ids_from_chars = ids_from_chars\n",
        "\n",
        "        # Create a mask to prevent \"[UNK]\" from being generated.\n",
        "        skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
        "        sparse_mask = tf.SparseTensor(\n",
        "            values=[-float('inf')] * len(skip_ids),\n",
        "            indices=skip_ids,\n",
        "            dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
        "        self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "    @tf.function\n",
        "    def generate_one_step(self, inputs, states=None):\n",
        "        input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "        input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "        predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
        "                                              return_state=True)\n",
        "        predicted_logits = predicted_logits[:, -1, :]\n",
        "        predicted_logits = predicted_logits / self.temperature\n",
        "\n",
        "        predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "        predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "        predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "        predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "        return predicted_chars, states\n",
        "\n"
      ],
      "metadata": {
        "id": "L0YeQebbliTI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lstm_model.summary()"
      ],
      "metadata": {
        "id": "ayYJNElGoMM2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an instance of the OneStepLSTM class\n",
        "one_step_model_lstm = OneStepLSTM(lstm_model, chars_from_ids, ids_from_chars)\n",
        "\n",
        "# Generate text using LSTM model\n",
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['Ron'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "    next_char, states = one_step_model_lstm.generate_one_step(next_char, states=states)\n",
        "    result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_' * 80)\n",
        "print('\\nRun time:', end - start)"
      ],
      "metadata": {
        "id": "uqEUVjtobrKd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model.compile(optimizer='adam', loss=loss)\n"
      ],
      "metadata": {
        "id": "7tSzyTXoliVw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save MyLSTMModel\n",
        "tf.saved_model.save(lstm_model, 'my_lstm_model')\n",
        "\n",
        "# Save OneStepLSTM\n",
        "one_step_lstm = OneStepLSTM(model=lstm_model, chars_from_ids=chars_from_ids, ids_from_chars=ids_from_chars)\n",
        "tf.saved_model.save(one_step_lstm, 'one_step_lstm_model')\n"
      ],
      "metadata": {
        "id": "F-tuN-Blj75O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adding Attention ***layer***"
      ],
      "metadata": {
        "id": "wTHB4T3yR8UV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# class MyModelWithAttention(tf.keras.Model):\n",
        "#     def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "#         super().__init__(self)\n",
        "#         self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "#         self.gru = tf.keras.layers.GRU(rnn_units,\n",
        "#                                        return_sequences=True,\n",
        "#                                        return_state=True)\n",
        "#         self.attention = AttentionLayer()  # Add attention layer\n",
        "#         self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "#     def call(self, inputs, states=None, return_state=False, training=False):\n",
        "#         x = inputs\n",
        "#         x = self.embedding(x, training=training)\n",
        "#         if states is None:\n",
        "#             states = self.gru.get_initial_state(x)\n",
        "#         x, states = self.gru(x, initial_state=states, training=training)\n",
        "\n",
        "#         # Apply attention layer\n",
        "#         attention_output = attention_3d_block(x)\n",
        "#         x = self.dense(attention_output, training=training)\n",
        "\n",
        "#         if return_state:\n",
        "#             return x, states\n",
        "#         else:\n",
        "#             return x\n"
      ],
      "metadata": {
        "id": "YApkaJL0igHF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class MyLSTMModelWithAttention(tf.keras.Model):\n",
        "#     def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "#         super().__init__(self)\n",
        "#         self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "#         self.lstm = tf.keras.layers.LSTM(rnn_units,\n",
        "#                                          return_sequences=True,\n",
        "#                                          return_state=True)\n",
        "#         self.attention = AttentionLayer()  # Add attention layer\n",
        "#         self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "#     def call(self, inputs, states=None, return_state=False, training=False):\n",
        "#         x = inputs\n",
        "#         x = self.embedding(x, training=training)\n",
        "#         if states is None:\n",
        "#             states = self.lstm.get_initial_state(x)\n",
        "#         x, states_h, states_c = self.lstm(x, initial_state=states, training=training)\n",
        "\n",
        "#         # Apply attention layer\n",
        "#         attention_output = attention_3d_block(x)\n",
        "#         x = self.dense(attention_output, training=training)\n",
        "\n",
        "#         if return_state:\n",
        "#             return x, [states_h, states_c]\n",
        "#         else:\n",
        "#             return x\n"
      ],
      "metadata": {
        "id": "3uFUX3iDpx9Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define parameters\n",
        "\n",
        "max_features   = 50000\n",
        "maxlen         = 200\n",
        "embedding_size = 128\n",
        "num_lstm_units = 256"
      ],
      "metadata": {
        "id": "SXZBv7rCq9iJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def attention_3d_block(hidden_states):\n",
        "  # Shape of hidden_states is (batch_size, seqlen, LSTM size)\n",
        "  hidden_size       = int(hidden_states.shape[2]) # LSTM size\n",
        "\n",
        "  # Create a dense layer for the attention score and fetch out the last hidden state\n",
        "  score_first_part  = Dense(hidden_size, use_bias=False, name='attention_score_vec')(hidden_states)\n",
        "  h_t               = Lambda(lambda x: x[:, -1, :], output_shape=(hidden_size,), name='last_hidden_state')(hidden_states)\n",
        "  # score_first_part shape: (batch_size, seqlen, LSTM size)\n",
        "  # h_t shape: (batch_size, LSTM size)\n",
        "\n",
        "  # Take the dot product of both, to get the final attention scores and push them through a softmax layer\n",
        "  score             = dot([score_first_part, h_t], [2, 1], name='attention_score')\n",
        "  attention_weights = Activation('softmax', name='attention_weight')(score)\n",
        "  # score shape: (batch_size, seqlen)\n",
        "  # attention_weights shape: (batch_size, seqlen)\n",
        "\n",
        "  # Take a dot product again to create a context vector\n",
        "  context_vector    = dot([hidden_states, attention_weights], [1, 1], name='context_vector')\n",
        "  # context_vector shape: (batch_size, LSTM)\n",
        "\n",
        "  # Add this context vector to h_t\n",
        "  pre_activation    = concatenate([context_vector, h_t], name='attention_output')\n",
        "  # pre_activation shape: (batch_size, LSTM*2)\n",
        "\n",
        "  # And create a final dense layer\n",
        "  attention_vector  = Dense(128, use_bias=False, activation='tanh', name='attention_vector')(pre_activation)\n",
        "  # attention_vector shape: (batch_size, 128)\n",
        "\n",
        "  return attention_vector"
      ],
      "metadata": {
        "id": "1q1Q5g4X0i43"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lstm_model.summary()"
      ],
      "metadata": {
        "id": "ax87Kr8f6YAs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define input shape\n",
        "input_seq = Input(shape=(maxlen,))\n",
        "\n",
        "# Define the embedding layer\n",
        "input_emb = Embedding(max_features, embedding_size)(input_seq)\n",
        "\n",
        "# Define the LSTM layers for both models\n",
        "lstm_model_emb = LSTM(num_lstm_units, dropout=0.2, recurrent_dropout=0.2, return_sequences=True)(input_emb)\n",
        "lstm_model_attention = attention_3d_block(lstm_model)\n",
        "\n",
        "lstm_model_with_attention = Model(inputs=input_seq, outputs=lstm_model_attention)\n",
        "\n",
        "# Compile the LSTM model with attention\n",
        "lstm_model_with_attention.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "# Define the LSTM model with attention\n",
        "lstm_model = LSTM(num_lstm_units, dropout=0.2, recurrent_dropout=0.2, return_sequences=True)(input_emb)\n",
        "lstm_model_attention = attention_3d_block(lstm_model)\n",
        "\n",
        "lstm_model_with_attention = Model(inputs=input_seq, outputs=lstm_model_attention)\n",
        "\n",
        "# Compile the LSTM model with attention\n",
        "lstm_model_with_attention.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n"
      ],
      "metadata": {
        "id": "DSCSxQ8sp4Ke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lstm_model_with_attention.summary()"
      ],
      "metadata": {
        "id": "6JqNsQ9z4Wlm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 20\n",
        "# Train the LSTM model with attention\n",
        "history_lstm_with_attention = lstm_model_with_attention.fit(X_train_input, X_train_target, epochs=epochs, validation_data=(X_test_input, X_test_target))\n"
      ],
      "metadata": {
        "id": "znAvKieMqvxS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def attention_3d_block(hidden_states):\n",
        "    # Shape of hidden_states is (batch_size, seqlen, LSTM size)\n",
        "    hidden_size = int(hidden_states.shape[2])  # LSTM size\n",
        "\n",
        "    # Create a dense layer for the attention score and fetch out the last hidden state\n",
        "    score_first_part = Dense(hidden_size, use_bias=False, name='attention_score_vec')(hidden_states)\n",
        "    h_t = Lambda(lambda x: x[:, -1, :], output_shape=(hidden_size,), name='last_hidden_state')(hidden_states)\n",
        "\n",
        "    # Take the dot product of both, to get the final attention scores and push them through a softmax layer\n",
        "    score = dot([score_first_part, h_t], [2, 1], name='attention_score')\n",
        "    attention_weights = Activation('softmax', name='attention_weight')(score)\n",
        "\n",
        "    # Take a dot product again to create a context vector\n",
        "    context_vector = dot([hidden_states, attention_weights], [1, 1], name='context_vector')\n",
        "\n",
        "    # Add this context vector to h_t\n",
        "    pre_activation = concatenate([context_vector, h_t], name='attention_output')\n",
        "\n",
        "    # And create a final dense layer\n",
        "    attention_vector = Dense(128, use_bias=False, activation='tanh', name='attention_vector')(pre_activation)\n",
        "\n",
        "    return attention_vector\n"
      ],
      "metadata": {
        "id": "HfGnQEeIT65k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create model with attention layer\n",
        "\n",
        "class MyModelWithAttention(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "        super().__init__(self)\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.gru = tf.keras.layers.GRU(rnn_units,\n",
        "                                       return_sequences=True,\n",
        "                                       return_state=True)\n",
        "        self.attention = attention_3d_block  # Add attention layer\n",
        "        self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    def call(self, inputs, states=None, return_state=False, training=False):\n",
        "        x = inputs\n",
        "        x = self.embedding(x, training=training)\n",
        "        if states is None:\n",
        "            states = self.gru.get_initial_state(x)\n",
        "        x, states = self.gru(x, initial_state=states, training=training)\n",
        "\n",
        "        # Apply attention layer\n",
        "        attention_output = self.attention(x)\n",
        "        x = self.dense(attention_output, training=training)\n",
        "\n",
        "        if return_state:\n",
        "            return x, states\n",
        "        else:\n",
        "            return x\n"
      ],
      "metadata": {
        "id": "0LKMUlOyql1w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vxzNPqfBqmx9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_seq = Input(shape=(maxlen))\n",
        "input_emb = Embedding(max_features, embedding_size)(input_seq)\n",
        "\n",
        "# Important: since we need to hidden states in our attention layer, we set return_sequences=True\n",
        "# return_sequences return the hidden state output for each input step\n",
        "# return_state returns the hidden state output and cell state for the last input step\n",
        "\n",
        "lstm = LSTM(num_lstm_units, dropout=0.2, recurrent_dropout=0.2, return_sequences=True)(input_emb)\n",
        "attention = attention_3d_block(lstm)\n",
        "\n",
        "# We force all weights here to be positive to make visualization of the attention layer easier\n",
        "dense = Dense(1, activation='sigmoid', use_bias=False, kernel_constraint=NonNeg())(Flatten()(attention))\n",
        "\n",
        "model = Model(inputs=input_seq, outputs=dense)\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n"
      ],
      "metadata": {
        "id": "9CIVH_AokFEz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an instance of the MyLSTMModelWithAttention class\n",
        "lstm_model_with_attention = MyLSTMModelWithAttention(\n",
        "    vocab_size=vocab_size,\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)\n",
        "\n",
        "# Compile the LSTM model with attention\n",
        "lstm_model_with_attention.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n"
      ],
      "metadata": {
        "id": "edMZSvQsXwwD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the LSTM model with attention\n",
        "history_lstm_with_attention = lstm_model_with_attention.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback_lstm])\n",
        "\n",
        "# Evaluate the LSTM model with attention\n",
        "test_loss_with_attention, test_accuracy_with_attention = lstm_model_with_attention.evaluate(dataset_test)\n",
        "print('Test loss with attention:', test_loss_with_attention)\n",
        "print('Test accuracy with attention:', test_accuracy_with_attention)"
      ],
      "metadata": {
        "id": "LQfBk-3DXx7C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get attention weights function\n",
        "get_attention_weights = K.function(inputs=lstm_model_with_attention.input, outputs=lstm_model_with_attention.get_layer(name='attention_weight').output)\n",
        "\n",
        "# Example usage of attention weights\n",
        "test_instance_idx = 7\n",
        "attention_weights = get_attention_weights(np.expand_dims(X_test[test_instance_idx], axis=0))\n",
        "\n"
      ],
      "metadata": {
        "id": "C5t25-QlrQRl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualization of attention weights\n",
        "cmap = cm.get_cmap('Reds')\n",
        "attention_normalized = np.expand_dims(minmax_scale(np.abs(attention_weights[0])), axis=0)\n",
        "\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.imshow(attention_normalized, cmap=cmap)\n",
        "\n"
      ],
      "metadata": {
        "id": "eHHeBy6mp7rV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of text highlighting with attention weights\n",
        "text = ''\n",
        "for i, w in enumerate(X_test[test_instance_idx]):\n",
        "    word = [k for k, v in imdb.get_word_index().items() if v == w][0] if w != 0 else '-----'\n",
        "    text += '<span style=\"background-color: {}\">{}</span> '.format(rgb2hex(cmap(attention_normalized[0, i])[:3]), word)\n",
        "\n",
        "HTML(text)\n"
      ],
      "metadata": {
        "id": "nfy18s5Dp9v7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}